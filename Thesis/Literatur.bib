%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for simon at 2024-08-22 12:19:54 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@url{mixamo,
	author = {Adobe},
	date-added = {2024-08-22 12:19:46 +0200},
	date-modified = {2024-08-22 12:19:52 +0200},
	title = {Mixamo},
	url = {https://www.mixamo.com/#/},
	bdsk-url-1 = {https://www.mixamo.com/#/}}

@url{sfu-motion-capture,
	author = {Goh Jing Ying, KangKang Yin, Karunanidhi Durai Kumar, Huang Geng, Sai Charan Mahadevan},
	date-added = {2024-08-21 11:45:45 +0200},
	date-modified = {2024-08-21 11:45:45 +0200},
	title = {SFU Motion Capture Database)},
	url = {https://mocap.cs.sfu.ca},
	bdsk-url-1 = {https://mocap.cs.sfu.ca}}

@article{10.1145/3355089.3356536,
	abstract = {Interactive control of self-balancing, physically simulated humanoids is a long standing problem in the field of real-time character animation. While physical simulation guarantees realistic interactions in the virtual world, simulated characters can appear unnatural if they perform unusual movements in order to maintain balance. Therefore, obtaining a high level of responsiveness to user control, runtime performance, and diversity has often been overlooked in exchange for motion quality. Recent work in the field of deep reinforcement learning has shown that training physically simulated characters to follow motion capture clips can yield high quality tracking results. We propose a two-step approach for building responsive simulated character controllers from unstructured motion capture data. First, meaningful features from the data such as movement direction, heading direction, speed, and locomotion style, are interactively specified and drive a kinematic character controller implemented using motion matching. Second, reinforcement learning is used to train a simulated character controller that is general enough to track the entire distribution of motion that can be generated by the kinematic controller. Our design emphasizes responsiveness to user input, visual quality, and low runtime cost for application in video-games.},
	address = {New York, NY, USA},
	articleno = {206},
	author = {Bergamin, Kevin and Clavet, Simon and Holden, Daniel and Forbes, James Richard},
	date-added = {2024-08-20 21:52:43 +0200},
	date-modified = {2024-08-20 21:52:43 +0200},
	doi = {10.1145/3355089.3356536},
	issn = {0730-0301},
	issue_date = {December 2019},
	journal = {ACM Trans. Graph.},
	keywords = {reinforcement learning, real-time graphics, physically based animation, motion capture},
	month = {nov},
	number = {6},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	title = {DReCon: data-driven responsive control of physics-based characters},
	url = {https://doi.org/10.1145/3355089.3356536},
	volume = {38},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3355089.3356536}}

@article{2022-TOG-ASE,
	address = {New York, NY, USA},
	articleno = {94},
	author = {Peng, Xue Bin and Guo, Yunrong and Halper, Lina and Levine, Sergey and Fidler, Sanja},
	date-added = {2024-08-20 21:48:28 +0200},
	date-modified = {2024-08-20 21:48:28 +0200},
	issue_date = {August 2022},
	journal = {ACM Trans. Graph.},
	keywords = {motion control, physics-based character animation, reinforcement learning},
	month = jul,
	number = {4},
	publisher = {ACM},
	title = {ASE: Large-scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters},
	volume = {41},
	year = {2022}}

@inproceedings{5333126,
	author = {Weyand, Peter G. and Smith, Bethany R. and Sandell, Rosalind F.},
	booktitle = {2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
	date-added = {2024-08-20 12:31:09 +0200},
	date-modified = {2024-08-20 12:31:09 +0200},
	doi = {10.1109/IEMBS.2009.5333126},
	keywords = {Costs;Legged locomotion;Humans;Animals;Biochemistry;Physiology;USA Councils;Velocity measurement;Helium;Energy measurement},
	pages = {6878-6881},
	title = {Assessing the metabolic cost of walking: The influence of baseline subtractions},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/IEMBS.2009.5333126}}

@url{aiwarehouse,
	author = {AI Warehouse},
	date-added = {2024-08-15 17:33:51 +0200},
	date-modified = {2024-08-16 12:05:21 +0200},
	title = {AI Learns to Walk (deep reinforcement learning)},
	url = {https://www.youtube.com/watch?v=L_4BPjLBF4E},
	year = {2023},
	bdsk-url-1 = {https://www.youtube.com/watch?v=L_4BPjLBF4E}}

@article{meyns2013and,
	author = {Meyns, Pieter and Bruijn, Sjoerd M and Duysens, Jacques},
	date-added = {2024-08-15 17:25:38 +0200},
	date-modified = {2024-08-15 17:25:38 +0200},
	journal = {Gait \& posture},
	number = {4},
	pages = {555--562},
	publisher = {Elsevier},
	title = {The how and why of arm swing during human walking},
	volume = {38},
	year = {2013}}

@article{drillis1964body,
	author = {Drillis, Rudolfs and Contini, Renato and Bluestein, Maurice},
	date-added = {2024-08-05 12:17:42 +0200},
	date-modified = {2024-08-05 12:17:42 +0200},
	journal = {Artificial limbs},
	number = {1},
	pages = {44--66},
	publisher = {Citeseer},
	title = {Body segment parameters},
	volume = {8},
	year = {1964}}

@article{peng2018deepmimic,
	author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Van de Panne, Michiel},
	date-added = {2024-07-28 12:12:33 +0200},
	date-modified = {2024-07-28 12:12:33 +0200},
	journal = {ACM Transactions On Graphics (TOG)},
	number = {4},
	pages = {1--14},
	publisher = {ACM New York, NY, USA},
	title = {Deepmimic: Example-guided deep reinforcement learning of physics-based character skills},
	volume = {37},
	year = {2018}}

@article{juliani2020,
	author = {Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},
	date-added = {2024-07-25 10:43:56 +0200},
	date-modified = {2024-07-25 10:43:56 +0200},
	journal = {arXiv preprint arXiv:1809.02627},
	title = {Unity: A general platform for intelligent agents},
	url = {https://arxiv.org/pdf/1809.02627.pdf},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/pdf/1809.02627.pdf}}

@article{schulman2017proximal,
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	date-added = {2024-07-23 14:11:58 +0200},
	date-modified = {2024-07-23 14:11:58 +0200},
	journal = {arXiv preprint arXiv:1707.06347},
	title = {Proximal policy optimization algorithms},
	year = {2017}}

@book{sutton2018reinforcement,
	author = {Sutton, Richard S and Barto, Andrew G},
	date-added = {2024-07-17 16:05:48 +0200},
	date-modified = {2024-07-17 16:05:48 +0200},
	publisher = {MIT press},
	title = {Reinforcement learning: An introduction},
	year = {2018}}
