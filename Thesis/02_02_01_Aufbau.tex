\subsection{Aufbau}
Das Toolkit ist in zwei Teile unterteilt (siehe Abbildung \ref{fig:mlagents_aufbau}). Für die Unity-Integration ist das Paket com.unity.ml-agents aus dem Unity Asset Store zuständig. Das eigentliche Training mit den maschinellen Lernalgorithmen findet jedoch in einer separaten Python-Umgebung statt. Für die Kommunikation zwischen den beiden Bereichen verwendet das ML-Agents Toolkit eine gRPC-Netzwerkkommunikation.\cite{juliani2020}

\begin{figure}[H]
  \centering  
  \begin{tikzpicture}[node distance=2cm]
  \node [rounded, draw=green, fill=green!30] (unity) {Unity Umgebung};
  \node [rounded, draw=red, fill=red!30, below of=unity] (python) {Python Umgebung};
  
  \draw [latex-latex, line width=0.3mm] (unity) -- (python);
  \end{tikzpicture}
  \caption{Unity ML-Agents Aufbau}
  \label{fig:mlagents_aufbau}
\end{figure}

Um eine Szene in Unity für das verstärkende Lernen zu nutzen, muss die Szene mindestens einen Agenten beinhalten. Jeder Agent referenziert ein Verhalten. Ein Verhalten kann eins von drei verschiedenen Modi verwenden. In Abbildung \ref{fig:mlagents_aufbau_unity} werden drei Agenten mit den unterschiedlichen Verhaltens-Modi dargestellt.

\begin{figure}[H]
  \centering  
  \begin{tikzpicture}[node distance=2cm]
  
    \node (agent1) [rounded, draw=green, fill=green!30] {Agent 1};
    \node (agent2) [rounded, right of=agent1, xshift=2cm, draw=green, fill=green!30] {Agent 2};
    \node (agent3) [rounded, right of=agent2, xshift=2cm, draw=green, fill=green!30] {Agent 3};
    
    \node (verhalten1) [rounded, below of=agent1 , draw=yellow, fill=yellow!30] {Verhalten Lernen};
    \node (verhalten2) [rounded, below of=agent2 , draw=yellow, fill=yellow!30] {Verhalten Inferenz};
    \node (verhalten3) [rounded, below of=agent3 , draw=yellow, fill=yellow!30] {Verhalten Heuristik};

    \node (kommunikator) [rounded, below of=verhalten1, draw=orange, fill=orange!60] {Kommunikator};
    \node (sentis) [rounded, below of=verhalten2, draw=orange, fill=orange!50] {Sentis};
    \node (heuristik) [rounded, below of=verhalten3 , draw=orange, fill=orange!30] {Heuristik};
    
    \draw [latex-latex, line width=0.3mm] (agent1)  -- (verhalten1);
    \draw [latex-latex, line width=0.3mm] (agent2)  -- (verhalten2);
    \draw [latex-latex, line width=0.3mm] (agent3)  -- (verhalten3);

    \draw [latex-latex, line width=0.3mm] (verhalten1)  -- (kommunikator);
    \draw [latex-latex, line width=0.3mm] (verhalten2) -- (sentis);
    \draw [latex-latex, line width=0.3mm] (verhalten3) -- (heuristik);
    
  \end{tikzpicture}
  \caption{Unity ML-Agents Aufbau Unity Umgebung \protect\cite{juliani2020}}
  \label{fig:mlagents_aufbau_unity}
\end{figure}

Das Verhalten bildet die Zuweisung von Beobachtung auf eine Aktion in der Unity Umgebung ab. Im Lernmodus nutzt es den Kommunikator, um in der Python Umgebung basierend auf der Beobachtung und der aktuellen Strategie eine Aktion auszuwählen. Im Inferenzmodus wird ein bereits trainiertes Modell mit dem Unity Sentis-Paket ausgeführt. Der Heuristikmodus wird meist zum Testen oder zum Aufzeichnen von Demonstrationen für das Imitationslernen verwendet. Die Heuristik verwendet fest kodierte Anweisungen, um beispielsweise die Aktionen über Tastatureingaben zu steuern.

\begin{figure}[H]
  \centering  
  \includegraphics[width=0.75\textwidth]{img/komponente_behavior_parameters}
  \caption{Unity ML-Agents Verhalten Parameter Komponente}
  \label{fig:komponente_behavior_parameters}
\end{figure}

Die Parameter aus Abbildung \ref{fig:komponente_behavior_parameters} werden in folgender Auflistung erklärt.

\begin{itemize}
  \item Behaviour Name: Name des Verhaltens / wird in Trainer Konfiguration referenziert
  \item Space Size: Anzahl an Beobachtungen / Inputknoten für NN
  \item Continuous Actions: Anzahl an Aktionen / Outputknoten von NN
  \item Model: Referenz auf bereits trainiertes Modell zur Verwendung in Inferenz
  \item Behaviour Type: Lernmodus Default = Lernen, Heuristic, Inferenz
\end{itemize}

Die Agent-Komponente bildet die Grundlage für alle Implementierungen. Sie bietet abstrakte Funktionen für die Initialisierung, den Start einer Episode, das Erfassen des Zustands der Umgebung sowie das Ausführen von Aktionen. Durch die Implementierung dieser Funktionen können unterschiedlichste Agenten entwickelt und trainiert werden. Die Beobachtungen des Agenten können auf zwei Arten erstellt werden. Beobachtungen basierend auf Raycasts sowie Kamerabildern werden mit separaten Komponenten erstellt. Beobachtungen aus Zahlenwerten sowie Vektoren und Quaternionen können jedoch auch direkt über die Beobachtungs-Funktion im Agenten der Beobachtung angehängt werden.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/komponente_agent}
  \caption{Unity ML-Agents Agenten Komponente}
  \label{fig:komponente_agent}
\end{figure}

Abbildung \ref{fig:komponente_agent} zeigt die Basiskomponente des Agenten. Ohne das Überschreiben der Funktionen ist die Agentenklasse jedoch ohne Funktion. Die genauen Methoden zur Implementierung eigener Agentenklassen werden im folgenden Abschnitt behandelt. Das einzige Feld zur Konfiguration ist \grqq{}Max Step\grqq{}, welches die maximale Anzahl der Schritte innerhalb einer Episode festlegt.

\begin{lstlisting}[caption={Agent Funktionen},captionpos=b,label={lst:agent_funktionen}]
public override void CollectObservations(VectorSensor sensor)
{
    sensor.AddObservation(floatObservation);
}

public override void OnActionReceived(ActionBuffers actionBuffers)
{
    var continuousActions = actionBuffers.ContinuousActions;
    movement.x += continuousActions[0]
    movement.y += continuousActions[1]
}

public virtual void FixedUpdate()
{
    AddReward(floatReward);
}
\end{lstlisting}

In der CollectObservations-Methode wird festgelegt, welche Daten dem Agenten für das Training bereitgestellt werden (siehe Listing \ref{lst:agent_funktionen} Zeile 1-3). CollectObservations wird für jede angefragte Entscheidung ausgeführt und das Ergebnis an das NN-Modell oder den Python Trainer übergeben. Wenn eine Entscheidung angefragt wurde und das NN-Modell ein Ergebnis liefert, wird dieses hier von numerischen Werten in Aktionen umgewandelt. In Listing \ref{lst:agent_funktionen} Zeile 6-11 wird gezeigt, wie die Aktion in X- und Y-Bewegung umgesetzt wird. Im Beispielcode in Listing \ref{lst:agent_funktionen} Zeile 13-16 wird eine Belohnung in jedem FixedUpdate vergeben, und zwar über die AddReward Methode, die auch Teil der Agentenkomponente ist. Die Belohnung kann aber an jeder Stelle im Code vergeben werden, der Code dient hier nur als ein Beispiel.

\begin{figure}[H]
  \centering  
  \includegraphics[width=0.75\textwidth]{img/komponente_decision_requester}
  \caption{Unity ML-Agents Entscheidung Anfragen Komponente}
  \label{fig:komponente_decision_requester}
\end{figure}

Die Komponente in Abbildung \ref{fig:komponente_decision_requester} fragt in regelmäßigen Abständen Entscheidungen an. Das bedeutet, es wird eine Beobachtung erstellt und darauf basierend eine Aktion über das Verhalten ausgewählt. Die \grqq{}Decision Period\grqq{} gibt an, in welchem Intervall der Agent eine Entscheidung treffen soll. Das Kontrollkästchen \grqq{}Take Actions Between Decisions\grqq{} gibt an, ob der Agent die ausgewählte Aktion wiederholen soll, bis die nächste Aktion ausgewählt wurde.