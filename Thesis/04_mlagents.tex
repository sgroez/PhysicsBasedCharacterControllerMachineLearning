{\chapter{Ml-Agents}}
\label{sec:mlagents}
Das Unity ML-Agents Toolkit ist ein Open-Source-Projekt, in dem maschinelle Lernalgorithmen und Funktionen für die Verwendung mit der Spieleumgebung Unity implementiert und kontinuierlich weiterentwickelt werden. Die Implementierung ist in zwei Bereiche unterteilt. Für die Unity-Integration ist das Paket com.unity.ml-agents aus dem Unity Asset Store zuständig. Das eigentliche Training mit den maschinellen Lernalgorithmen findet jedoch in einer separaten Python-Umgebung statt. Für die Kommunikation zwischen den beiden Bereichen verwendet das ML-Agents Toolkit eine C\# Kommunikator-Klasse, die über gRPC-Netzwerkkommunikation mit dem Python-Prozess kommuniziert. Der Python-Prozess kommuniziert über die Python Low-Level-API, die die Kommunikation übernimmt und die Befehle an den Trainer weiterleitet.\cite{unity_mlagents_toolkit_overview}

\begin{figure}[H]
  \centering  
  \includegraphics[scale=0.4]{img/learning_environment_basic.png}
  \caption{Unity ML-Agents Lernumgebung \protect\cite{unity_mlagents_learning_environment_basic}}
  \label{fig:learning_environment_basic}
\end{figure}

Das Unity-Paket enthält zwei Komponenten: Agenten und deren Verhalten. Die Agent-Komponente bildet die Grundlage für alle Implementierungen. Sie bietet abstrakte Funktionen für die Initialisierung, den Start einer Episode, das Erfassen des Zustands der Umgebung sowie das Ausführen von Aktionen. Durch die Implementierung dieser Funktionen können unterschiedlichste Agenten entwickelt und trainiert werden. Jeder Agent ist mit einem Verhalten verknüpft, das für jede Beobachtung des Agenten eine Aktion auswählt, die der Agent ausführt. Es gibt drei Arten, wie die Verhaltensweisen agieren können. Im Lernmodus werden die Beobachtungen des Agenten für das Training und die Auswahl einer Aktion anhand des aktuellen Modells verwendet. Der Inferenzmodus nutzt hingegen ein bereits trainiertes Modell und wertet dieses aus. Der letzte Modus eines Verhaltens ist der Heuristikmodus, bei dem festgelegte Regeln im Code entscheiden, welche Aktion ausgeführt wird, ohne die Verwendung eines trainierten Modells.\cite{unity_mlagents_toolkit_overview}

\begin{figure}[H]
  \centering  
  \includegraphics[scale=0.3]{img/learning_environment_example.png}
  \caption{Unity ML-Agents Lernumgebung Beispiel \protect\cite{unity_mlagents_learning_environment_example}}
  \label{fig:learning_environment_example}
\end{figure}

\section{Komponenten}
In diesem Kapitel werde Ich die Grundlegenden Komponenten des Unity ML-Agents Packets, welche in der Arbeit verwendet wurden erklären. Darurch sollten Codeausschnitte und Komponentenabbildungen in folgenden Kapiteln deutlich zu verstehen sein.

\subsection{Verhalten}
\begin{figure}[H]
  \centering  
  \includegraphics[scale=0.5]{img/verhalten_komponente.png}
  \caption{Unity ML-Agents Verhalten Komponente}
  \label{fig:verhalten_komponente}
\end{figure}

\begin{center}
{\rowcolors{2}{lightgray}{gray!50!lightgray!50}
\begin{tabular}{ |p{4cm}|p{8cm}| }
\hline
Konfigurationsfeld& Beschreibung \\
\hline
Behaviour Name & Name des Verhaltens / wird in Trainer Konfiguration referenziert \\
Space Size & Anzahl an Beobachtungen / Inputknoten für NN \\
Continuous Actions & Anzahl an Aktionen / Outputknoten von NN \\
Model & Referenz auf bereits trainiertes Modell zur Verwendung in Inferenz \\
Behaviour Type & Lernmodus Default = Lernen, Heuristic, Inferenz \\
\hline
\end{tabular}}
\end{center}

\subsection{Entscheidung}
\begin{figure}[H]
  \centering  
  \includegraphics[scale=0.5]{img/entscheidung_anfragen_komponente.png}
  \caption{Unity ML-Agents Entscheidung Anfragen Komponente}
  \label{fig:entscheidung_anfragen_komponente}
\end{figure}

\begin{center}
{\rowcolors{2}{lightgray}{gray!50!lightgray!50}
\begin{tabular}{ |p{4cm}|p{8cm}| }
\hline
Konfigurationsfeld& Beschreibung \\
\hline
Decision Period & Anzahl an Akademie-Schritten (standard ein Schritt pro Physikupdate) bis zur nächsten Entscheidung \\
Take Actions Between Decisions &  Kontrollkasten ob Agent Aktionen zwischen Entscheidungen ausführen soll \\
\hline
\end{tabular}}
\end{center}


\subsection{Agent Abstrakte Funktionen}
Die Akademie stellt mit dem Attribut EnvironmentParameters die Umgebungsparameter aus Trainer Konfiguration oder aktueller Lektion bereit
\begin{lstlisting}
envParams = Academy.Instance.EnvironmentParameters;
\end{lstlisting}

Mit dem StatsRecorder lassen sich Daten aggregieren um diese nach oder während dem Training über die Tensorboard Visualisierung auszuwerten
\begin{lstlisting}
statsRecorder = Academy.Instance.StatsRecorder;
\end{lstlisting}

In der CollectObservations Methoden wird festgelegt welche Daten dem Agent für das Training bereit stehen, dieser Schritt wird für jede angefragte Entscheidung ausgeführt und das Ergebnis an das NN Modell oder den Python Trainer übergeben.
\begin{lstlisting}
public override void CollectObservations(VectorSensor sensor)
{
    sensor.AddObservation(floatObservation);
}
\end{lstlisting}

Wenn eine Entscheidung angefragt wurde und das NN Modell ein Ergebnis liefert wird dieses hier von numerischen Werten in Aktionen umgewandelt.
\begin{lstlisting}
public override void OnActionReceived(ActionBuffers actionBuffers)
{
    var continuousActions = actionBuffers.ContinuousActions;
    float action = continuousActions[0]
}
\end{lstlisting}

Im folgenden Beispielcode wird ein Reward in jedem FixedUpdate vergeben über die AddReward Methode die auch Teil der Agenten-Komponente ist. Der Reward kann aber an jeder Stelle im Code vergeben werden, der Code dient hier nur als ein Beispiel.
\begin{lstlisting}
public virtual void FixedUpdate()
{
    AddReward(floatReward);
}
\end{lstlisting}

Die Trainings Konfigurationdatei enthält mehrere Teile. Der hyperparameter Teil enthält die Hyperparameter des Maschinellen Lernalgorithmuses, danach folgt der network\_settings Teil welcher die Konfiguration des Neuronalennetzes festlegt. Anschließend folgen noch Konfigurationen für die Belohnungssignale im Bereich reward\_signals und Einstellungen für die Speicherung der Daten sowie der länge des Trainings. Ganz am Ende der Konfigurationsdatei befinden sich noch Umgebungsparameter welche erweitert und während dem Training ausgelesen werden können.
\begin{lstlisting}
{
behaviors:
  Walker:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    keep_checkpoints: 5
    checkpoint_interval: 5000000
    max_steps: 30000000
    time_horizon: 1000
    summary_freq: 30000
environment_parameters:
  environment_count: 100.0
}
\end{lstlisting}
